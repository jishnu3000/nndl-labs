{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "019e3d70",
      "metadata": {
        "id": "019e3d70"
      },
      "source": [
        "# Lab 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4342f80",
      "metadata": {
        "id": "b4342f80"
      },
      "source": [
        "## Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a28c47",
      "metadata": {
        "id": "c2a28c47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2505644f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2505644f",
        "outputId": "77a58f6b-3f92-4895-80ef-195fa612d4b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3464,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 0,\n        \"max\": 199,\n        \"num_unique_values\": 200,\n        \"samples\": [\n          155,\n          88,\n          37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3406,\n        \"samples\": [\n          \"\\r\\r\\n                    The Violent Space (or when your sister sleeps around for money)\\r\\r\\n                \",\n          \"\\r\\r\\n                    Natural History\\r\\r\\n                \",\n          \"\\r\\r\\n                    Amoretti I: Happy ye leaves when as those lilly hands\\r\\r\\n                \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Poem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3448,\n        \"samples\": [\n          \"\\r\\r\\nWhatever city or country road   \\r\\r\\nyou two are on\\r\\r\\nthere are nettles,\\r\\r\\nand the dark invisible\\r\\r\\nelements cling to your skin\\r\\r\\nthough you do not cry\\r\\r\\nand you do not scratch\\r\\r\\nyour arms at forty-five degree angles   \\r\\r\\nas the landing point of a swan\\r\\r\\nin the Ohio, the Detroit River;\\r\\r\\nat the Paradise Theatre\\r\\r\\nyou named the cellist\\r\\r\\nwith the fanatical fingers\\r\\r\\nof the plumber, the exorcist,\\r\\r\\nand though the gimmicky at wrist   \\r\\r\\nand kneecaps could lift the seance   \\r\\r\\ntable, your voice was real\\r\\r\\nin the gait and laughter of Uncle   \\r\\r\\nHenry, who could dance on either   \\r\\r\\nleg, wooden or real, to the sound   \\r\\r\\nof the troop train, megaphone,   \\r\\r\\ncatching the fine pitch of a singer   \\r\\r\\non the athletic fields of Virginia.\\r\\r\\nAt the Radisson Hotel,\\r\\r\\nwe once took a fine angel\\r\\r\\nof the law to the convention center,   \\r\\r\\nand put her down as an egret\\r\\r\\nin the subzero platform of a friend\\u2014\\r\\r\\nthis is Minneapolis, the movies\\r\\r\\nare all of strangers, holding themselves   \\r\\r\\nin the delicacy of treading water,   \\r\\r\\nwhile they wait for the trumpet   \\r\\r\\nof the 20th Century Limited\\r\\r\\nover the bluff or cranny.\\r\\r\\nYou two men like to confront.\\r\\r\\nthe craters of history and spillage,   \\r\\r\\nour natural infections of you   \\r\\r\\ninnoculating blankets and fur,   \\r\\r\\nethos of cadaver and sunflower.\\r\\r\\nI hold the dogwood blossom,\\r\\r\\neat the pear, and watch the nettle   \\r\\r\\nswim up in the pools\\r\\r\\nof the completed song\\r\\r\\nof Leadbelly and Little Crow   \\r\\r\\ncrooning the buffalo and horse   \\r\\r\\nto the changes and the bridge   \\r\\r\\nof a twelve-string guitar,\\r\\r\\nthe melody of \\u201cIrene\\u201d;\\r\\r\\nthis is really goodbye\\u2014\\r\\r\\nI can see the precious stones\\r\\r\\nof embolism and consumption\\r\\r\\non the platinum wires of the mouth:\\r\\r\\nin the flowing rivers, in the public baths   \\r\\r\\nof Ohio and Michigan.\\r\\r\\n\",\n          \"\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nCamille Corot's painting, stolen from the Louvre, May 1998\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nIt might have always been meant  that they walked completely away,  this man on horse, woman with basket.  With their backs to us and the painter,  they are so private. But like those stories  where children step right into a picture  and, looking over their shoulders,  see the consoling frame,  these two would know the way home  like the palms of their hands, the routine  so ordinary it most encloses,  no need for thought, only motion  and the full sensation of sun  on your flesh, along the usual road.\\r\\r\\n\",\n          \"\\r\\r\\nYou whom I could not save\\r\\r\\nListen to me.   \\r\\r\\nTry to understand this simple speech as I would be ashamed of another.   \\r\\r\\nI swear, there is in me no wizardry of words.   \\r\\r\\nI speak to you with silence like a cloud or a tree.\\r\\r\\nWhat strengthened me, for you was lethal.   \\r\\r\\nYou mixed up farewell to an epoch with the beginning of a new one,   \\r\\r\\nInspiration of hatred with lyrical beauty;   \\r\\r\\nBlind force with accomplished shape.\\r\\r\\nHere is a valley of shallow Polish rivers. And an immense bridge   \\r\\r\\nGoing into white fog. Here is a broken city;   \\r\\r\\nAnd the wind throws the screams of gulls on your grave   \\r\\r\\nWhen I am talking with you.\\r\\r\\nWhat is poetry which does not save   \\r\\r\\nNations or people?   \\r\\r\\nA connivance with official lies,   \\r\\r\\nA song of drunkards whose throats will be cut in a moment,   \\r\\r\\nReadings for sophomore girls.\\r\\r\\nThat I wanted good poetry without knowing it,   \\r\\r\\nThat I discovered, late, its salutary aim,   \\r\\r\\nIn this and only this I find salvation.\\r\\r\\nThey used to pour millet on graves or poppy seeds   \\r\\r\\nTo feed the dead who would come disguised as birds.   \\r\\r\\nI put this book here for you, who once lived   \\r\\r\\nSo that you should visit us no more.   Warsaw, 1945\\r\\r\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Poet\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1669,\n        \"samples\": [\n          \"Justin Phillip Reed\",\n          \"Mahmoud Darwish\",\n          \"Henri Cole\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tags\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2467,\n        \"samples\": [\n          \"Activities,Eating & Drinking,Relationships,Friends & Enemies,Christmas\",\n          \"Living,Parenthood,Separation & Divorce,Activities,Eating & Drinking,Relationships,Home Life\",\n          \"Love,Relationships,Arts & Sciences,Humor & Satire,Poetry & Poets,Reading & Books\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fe793d25-4729-43df-9ae1-fdec560bb2b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Title</th>\n",
              "      <th>Poem</th>\n",
              "      <th>Poet</th>\n",
              "      <th>Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>77</td>\n",
              "      <td>\\r\\r\\n                    At Eighty-three She ...</td>\n",
              "      <td>\\r\\r\\nEnclosure, steam-heated; a trial casket....</td>\n",
              "      <td>Ruth Stone</td>\n",
              "      <td>Living,Growing Old,Arts &amp; Sciences</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "      <td>\\r\\r\\n                    And the Gauchos Sing...</td>\n",
              "      <td>\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nFor Barry Silesky\\r\\r\\...</td>\n",
              "      <td>Mike Puican</td>\n",
              "      <td>Arts &amp; Sciences,Poetry &amp; Poets,Social Commenta...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29</td>\n",
              "      <td>\\r\\r\\nfrom Saying Grace\\r\\r\\n</td>\n",
              "      <td>\\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nfor my mother\\r\\r\\n\\r\\...</td>\n",
              "      <td>Kevin Young</td>\n",
              "      <td>Activities,Jobs &amp; Working,Social Commentaries,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>78</td>\n",
              "      <td>\\r\\r\\n                    Leaving the Hospital...</td>\n",
              "      <td>\\r\\r\\nAs the doors glide shut behind me, the w...</td>\n",
              "      <td>Anya Silver</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13</td>\n",
              "      <td>\\r\\r\\n                    Relic\\r\\r\\n         ...</td>\n",
              "      <td>\\r\\r\\nThe first time I touched it, cloth fell ...</td>\n",
              "      <td>Rachel Richardson</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe793d25-4729-43df-9ae1-fdec560bb2b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe793d25-4729-43df-9ae1-fdec560bb2b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe793d25-4729-43df-9ae1-fdec560bb2b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-31337edc-2237-489c-8f33-1ff2e11f8921\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-31337edc-2237-489c-8f33-1ff2e11f8921')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-31337edc-2237-489c-8f33-1ff2e11f8921 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Unnamed: 0                                              Title  \\\n",
              "0          77  \\r\\r\\n                    At Eighty-three She ...   \n",
              "1           6  \\r\\r\\n                    And the Gauchos Sing...   \n",
              "2          29      \\r\\r\\nfrom Saying Grace\\r\\r\\n                   \n",
              "3          78  \\r\\r\\n                    Leaving the Hospital...   \n",
              "4          13  \\r\\r\\n                    Relic\\r\\r\\n         ...   \n",
              "\n",
              "                                                Poem               Poet  \\\n",
              "0  \\r\\r\\nEnclosure, steam-heated; a trial casket....         Ruth Stone   \n",
              "1  \\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nFor Barry Silesky\\r\\r\\...        Mike Puican   \n",
              "2  \\r\\r\\n\\r\\r\\n\\r\\r\\n\\r\\r\\nfor my mother\\r\\r\\n\\r\\...        Kevin Young   \n",
              "3  \\r\\r\\nAs the doors glide shut behind me, the w...        Anya Silver   \n",
              "4  \\r\\r\\nThe first time I touched it, cloth fell ...  Rachel Richardson   \n",
              "\n",
              "                                                Tags  \n",
              "0                 Living,Growing Old,Arts & Sciences  \n",
              "1  Arts & Sciences,Poetry & Poets,Social Commenta...  \n",
              "2  Activities,Jobs & Working,Social Commentaries,...  \n",
              "3                                                NaN  \n",
              "4                                                NaN  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('PoetryFoundationData.csv')\n",
        "df = df.sample(frac=0.25, random_state=42).reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "604c7e39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "604c7e39",
        "outputId": "31331c1d-9fdf-4003-868c-80905536a7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3464 entries, 0 to 3463\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   Unnamed: 0  3464 non-null   int64 \n",
            " 1   Title       3464 non-null   object\n",
            " 2   Poem        3464 non-null   object\n",
            " 3   Poet        3464 non-null   object\n",
            " 4   Tags        3224 non-null   object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 135.4+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1304634",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1304634",
        "outputId": "d0e35032-d711-4f81-c62b-272de8818b74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus created with 3443 poems (4964851 characters). Saved to poems_corpus.txt\n"
          ]
        }
      ],
      "source": [
        "# normalize poem line endings, strip leading/trailing whitespace, drop empty entries\n",
        "poems = df['Poem'].astype(str).map(lambda s: s.replace('\\r\\n', '\\n').replace('\\r', '\\n').strip())\n",
        "poems = poems[poems != '']\n",
        "\n",
        "# join poems into a single corpus, separating poems by two newlines for clarity\n",
        "corpus = '\\n\\n'.join(poems.tolist())\n",
        "\n",
        "# (optional) save corpus to a file\n",
        "with open('poems_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "  f.write(corpus)\n",
        "\n",
        "print(f'Corpus created with {len(poems)} poems ({len(corpus)} characters). Saved to poems_corpus.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f41974c",
      "metadata": {
        "id": "8f41974c"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee7accb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee7accb",
        "outputId": "403d8993-ae7f-498d-8838-567708dd8ea1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocab_size=48741, num_sequences=844950, X.shape=(844950, 5), y.shape=(844950,)\n",
            "example (first seq): enclosure steam heated a trial -> casket\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Parameters\n",
        "n = 5  # context length (use n words to predict the (n+1)th)\n",
        "oov_tok = '<OOV>'\n",
        "\n",
        "# 1) Clean: lowercase and remove punctuation/special chars\n",
        "def clean_text(s: str) -> str:\n",
        "  s = s.lower()\n",
        "  # replace newlines with spaces, drop non-alphanum (keep digits and spaces)\n",
        "  s = re.sub(r'[\\r\\n]+', ' ', s)\n",
        "  s = re.sub(r'[^a-z0-9\\s]', ' ', s)\n",
        "  s = re.sub(r'\\s+', ' ', s).strip()\n",
        "  return s\n",
        "\n",
        "cleaned_poems = poems.astype(str).map(clean_text).tolist()\n",
        "\n",
        "# 2) Tokenize (word -> integer)\n",
        "tokenizer = Tokenizer(oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(cleaned_poems)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding / index start\n",
        "\n",
        "# 3) Create sliding-window sequences (n inputs + 1 target)\n",
        "sequences = []\n",
        "for text in cleaned_poems:\n",
        "  token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "  if len(token_list) <= n:\n",
        "    continue\n",
        "  for i in range(n, len(token_list)):\n",
        "    seq = token_list[i - n : i + 1]  # length = n+1\n",
        "    sequences.append(seq)\n",
        "\n",
        "if len(sequences) == 0:\n",
        "  raise ValueError(\"No sequences were created. Try reducing `n` or check cleaned_poems content.\")\n",
        "\n",
        "sequences = np.array(sequences, dtype=np.int32)\n",
        "\n",
        "# 4) Pad sequences so they all have same length (here maxlen = n+1)\n",
        "max_len = n + 1\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "# Split into inputs and targets\n",
        "X = padded_sequences[:, :-1]  # shape (num_seqs, n)\n",
        "y = padded_sequences[:, -1]   # shape (num_seqs,)\n",
        "\n",
        "# Quick summary\n",
        "print(f\"vocab_size={vocab_size}, num_sequences={X.shape[0]}, X.shape={X.shape}, y.shape={y.shape}\")\n",
        "# example: decode one X -> words\n",
        "idx_to_word = {i: w for w, i in tokenizer.word_index.items()}\n",
        "def decode_tokens(tokens):\n",
        "  return \" \".join(idx_to_word.get(t, '<PAD_OR_OOV>') for t in tokens)\n",
        "\n",
        "print(\"example (first seq):\", decode_tokens(X[0]), \"->\", idx_to_word.get(y[0], '<PAD_OR_OOV>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145bc956",
      "metadata": {
        "id": "145bc956"
      },
      "source": [
        "## LSTM Model Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397f850f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "397f850f",
        "outputId": "a467db2e-3c0a-424f-96ec-9c92f75ce877"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# configuration\n",
        "embedding_dim = 100\n",
        "lstm_units = 100\n",
        "dropout_rate = 0.2\n",
        "\n",
        "# build model (two LSTM layers)\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=n))\n",
        "model.add(LSTM(lstm_units, return_sequences=True))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dropout(dropout_rate))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7cf55b9",
      "metadata": {
        "id": "f7cf55b9"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cc7806b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cc7806b",
        "outputId": "101de32a-089b-4ae6-acb9-fd372f9ab3bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 81ms/step - accuracy: 0.0614 - loss: 7.7462\n",
            "Epoch 2/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.0780 - loss: 6.9579\n",
            "Epoch 3/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.0936 - loss: 6.7268\n",
            "Epoch 4/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 81ms/step - accuracy: 0.1006 - loss: 6.5607\n",
            "Epoch 5/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 81ms/step - accuracy: 0.1063 - loss: 6.4204\n",
            "Epoch 6/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - accuracy: 0.1110 - loss: 6.2936\n",
            "Epoch 7/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - accuracy: 0.1151 - loss: 6.1749\n",
            "Epoch 8/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - accuracy: 0.1182 - loss: 6.0599\n",
            "Epoch 9/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.1214 - loss: 5.9497\n",
            "Epoch 10/10\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.1247 - loss: 5.8428\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import json\n",
        "\n",
        "# Recompile model to use categorical cross-entropy and standard accuracy\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create a tf.data pipeline that one-hot encodes targets per-batch (avoids materializing a gigantic y matrix)\n",
        "batch_size = 512\n",
        "shuffle_buffer = 10000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "dataset = dataset.shuffle(shuffle_buffer).map(lambda x, y: (x, tf.one_hot(y, depth=vocab_size, dtype=tf.float32)))\n",
        "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Train the model (adjust epochs between 10-20 as desired)\n",
        "epochs = 10\n",
        "history = model.fit(dataset, epochs=epochs)\n",
        "\n",
        "# Save training history\n",
        "with open('training_history.json', 'w') as fh:\n",
        "  json.dump(history.history, fh)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39daf064",
      "metadata": {
        "id": "39daf064"
      },
      "source": [
        "## Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc690aa0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc690aa0",
        "outputId": "5ab7c991-b707-4103-af63-37cb93a3040c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> the moon\n",
            "the moon rising that love and all that i have lost the tv no\n",
            "\n",
            "> i remember\n",
            "i remember no other but a way about the story of the sun so\n",
            "\n",
            "> dear heart\n",
            "dear heart is it here i think of the world i will be wondering\n",
            "\n",
            "> in the morning light\n",
            "in the morning light from his fingers the children that is a dusty view where she\n",
            "\n",
            "> under the old oak\n",
            "under the old oak a stone near a nanda s fine garment a web to read\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def _sample_from_preds(preds, temperature=1.0):\n",
        "  preds = np.asarray(preds, dtype=np.float64)\n",
        "  preds[0] = 0.0  # never predict padding index 0\n",
        "  if temperature <= 0 or np.isclose(temperature, 0.0):\n",
        "    return int(np.argmax(preds))\n",
        "  preds = preds / np.sum(preds)\n",
        "  # apply temperature by working in log-space to avoid underflow\n",
        "  log_preds = np.log(preds + 1e-12) / temperature\n",
        "  exp_preds = np.exp(log_preds - np.max(log_preds))\n",
        "  probs = exp_preds / np.sum(exp_preds)\n",
        "  return int(np.random.choice(len(probs), p=probs))\n",
        "\n",
        "def generate_poetry(seed: str,\n",
        "          num_words: int = 20,\n",
        "          temperature: float = 1.0,\n",
        "          sample: bool = True) -> str:\n",
        "  \"\"\"\n",
        "  Generate a single line/sequence of `num_words` words continuing from `seed`.\n",
        "  Uses `tokenizer`, `model`, `n`, and `idx_to_word` from the notebook state.\n",
        "  \"\"\"\n",
        "  # clean + tokenize seed using the same preprocessing used for training\n",
        "  cleaned = clean_text(seed)\n",
        "  token_list = tokenizer.texts_to_sequences([cleaned])[0]  # may be empty or contain OOV index\n",
        "  generated_tokens = token_list.copy()\n",
        "\n",
        "  for _ in range(num_words):\n",
        "    # take last `n` tokens as context, pad on the left if needed\n",
        "    input_seq = pad_sequences([generated_tokens[-n:]], maxlen=n, padding='pre')\n",
        "    preds = model.predict(input_seq, verbose=0)[0]  # shape (vocab_size,)\n",
        "    if sample:\n",
        "      next_idx = _sample_from_preds(preds, temperature=temperature)\n",
        "    else:\n",
        "      preds[0] = 0.0\n",
        "      next_idx = int(np.argmax(preds))\n",
        "    generated_tokens.append(next_idx)\n",
        "\n",
        "  # convert token indices back to words (skip padding 0 tokens)\n",
        "  words = [idx_to_word.get(t, '<OOV>') for t in generated_tokens if t != 0]\n",
        "  return \" \".join(words)\n",
        "\n",
        "def generate_multiple_lines(seeds, num_words=15, temperature=1.0, sample=True):\n",
        "  \"\"\"\n",
        "  Generate multiple lines from a list of seeds. Returns a list of strings.\n",
        "  \"\"\"\n",
        "  lines = []\n",
        "  for s in seeds:\n",
        "    line = generate_poetry(s, num_words=num_words, temperature=temperature, sample=sample)\n",
        "    lines.append(line)\n",
        "  return lines\n",
        "\n",
        "# Example usage (change seeds and parameters as desired):\n",
        "example_seeds = [\n",
        "  \"the moon\",\n",
        "  \"i remember\",\n",
        "  \"dear heart\",\n",
        "  \"in the morning light\",\n",
        "  \"under the old oak\"\n",
        "]\n",
        "\n",
        "generated = generate_multiple_lines(example_seeds, num_words=12, temperature=0.8, sample=True)\n",
        "for seed, line in zip(example_seeds, generated):\n",
        "  print(f\"> {seed}\\n{line}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4787c106",
      "metadata": {
        "id": "4787c106"
      },
      "source": [
        "## Evaluation and Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "13728e3a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13728e3a",
        "outputId": "31522fbe-d74c-4cea-8f9a-7c643c5fea09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Starting experiment: ctx3_L128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 81ms/step - accuracy: 0.0623 - loss: 7.7824\n",
            "Epoch 2/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - accuracy: 0.0887 - loss: 6.8391\n",
            "Epoch 3/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 81ms/step - accuracy: 0.1010 - loss: 6.5955\n",
            "Epoch 4/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.1100 - loss: 6.3902\n",
            "Finished ctx3_L128_do0.1 in 547.7s. Last-epoch loss=6.3364\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon s a likeness i ve seen the human eyes i am i say not do you have been done you\n",
            "    [i remember] -> i remember up as the snail has been the fool you are on the sun and the dark the ones grow as\n",
            "    [in the morning] -> in the morning what makes me see me for you to make the sky she s just to write the world of common\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon when in a rage he laughed as o er why when i don t love thinking for his brothers noise\n",
            "    [i remember] -> i remember the world knows the spring and i face your fathers aspect of the dark fingers through the row of a\n",
            "    [in the morning] -> in the morning i piece part refuse to your armpits and sandy nak and the pocks which can think you make one the\n",
            "================================================================================\n",
            "Starting experiment: ctx3_L128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 81ms/step - accuracy: 0.0618 - loss: 7.7947\n",
            "Epoch 2/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 81ms/step - accuracy: 0.0860 - loss: 6.9046\n",
            "Epoch 3/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - accuracy: 0.0994 - loss: 6.6800\n",
            "Epoch 4/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 81ms/step - accuracy: 0.1075 - loss: 6.5171\n",
            "Finished ctx3_L128_do0.3 in 554.3s. Last-epoch loss=6.4689\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon s paper and the then that is it just the late of her own and i love the world on\n",
            "    [i remember] -> i remember you can t have not been we have just been in a arm of the sky i ll never get\n",
            "    [in the morning] -> in the morning i am a story of a end of the world who is a string of a goat was a man\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon to heap their glass both as the hills where which are goodbye once it were brown in a shelf i\n",
            "    [i remember] -> i remember how my love i have just in africa and your further fog bring through the moon of the wind above\n",
            "    [in the morning] -> in the morning because the sea are miss the hillside how made equally the corpse society out the pitch of her hand who\n",
            "================================================================================\n",
            "Starting experiment: ctx3_L128-128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 81ms/step - accuracy: 0.0619 - loss: 7.7727\n",
            "Epoch 2/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 81ms/step - accuracy: 0.0820 - loss: 6.8898\n",
            "Epoch 3/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 81ms/step - accuracy: 0.0992 - loss: 6.6091\n",
            "Epoch 4/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 81ms/step - accuracy: 0.1072 - loss: 6.4078\n",
            "Finished ctx3_L128-128_do0.1 in 550.4s. Last-epoch loss=6.3563\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon was a woman or sisters now or the surface of the house i was in the dust of the street\n",
            "    [i remember] -> i remember it is a most man a poet is a third and a woman s mind i know to make her\n",
            "    [in the morning] -> in the morning a place of the same way and the lashes all i am not a way at college she ll never\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon eventually that picks him into the dog claim in melting books suddenly a long slop in on air kept like\n",
            "    [i remember] -> i remember a friend and as i am sure i have like one times are freed with lovers the concealing monarchs as\n",
            "    [in the morning] -> in the morning on the neon day of sound drill i feel you though me could never make her heart the year s\n",
            "================================================================================\n",
            "Starting experiment: ctx3_L128-128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 82ms/step - accuracy: 0.0610 - loss: 7.7856\n",
            "Epoch 2/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 82ms/step - accuracy: 0.0793 - loss: 6.9453\n",
            "Epoch 3/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.0964 - loss: 6.6931\n",
            "Epoch 4/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.1040 - loss: 6.5222\n",
            "Finished ctx3_L128-128_do0.3 in 558.7s. Last-epoch loss=6.4781\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon of a man and like the afternoon below a series of course now i m no way to go to\n",
            "    [i remember] -> i remember the city and the year is a good man s man i won t know but if but i ve\n",
            "    [in the morning] -> in the morning a sun that s blue of the gold we know you were a day in the body he was going\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon of our country plastic breasts door my beauty never knew but thou cleared to pull thee out as the tortoise\n",
            "    [i remember] -> i remember remember where are caught i m to go to him twenty of the granite i ve not save the mountains\n",
            "    [in the morning] -> in the morning s throats or all maybe i can not build us don t keep us you ll be that is a\n",
            "================================================================================\n",
            "Starting experiment: ctx3_L256-128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 82ms/step - accuracy: 0.0615 - loss: 7.7502\n",
            "Epoch 2/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 82ms/step - accuracy: 0.0879 - loss: 6.8371\n",
            "Epoch 3/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 82ms/step - accuracy: 0.1009 - loss: 6.5749\n",
            "Epoch 4/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 82ms/step - accuracy: 0.1088 - loss: 6.3662\n",
            "Finished ctx3_L256-128_do0.1 in 552.4s. Last-epoch loss=6.3117\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon in the time a man could not have the very storm of my clothes and i am the man s\n",
            "    [i remember] -> i remember the pleasure of a judgment to my own hand and to the path of a trance of way to our\n",
            "    [in the morning] -> in the morning i said the name and the world s a young man i would never be a man of light all\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon watching the world with gobshites on pink spar to sound as bulges for diamantine american lady i am not as\n",
            "    [i remember] -> i remember the dead s silver cruz beak up a splintered three wings might not be ya he are with the bay\n",
            "    [in the morning] -> in the morning the city of dew is one as i know but a dirt to the inhabitants of what it begins to\n",
            "================================================================================\n",
            "Starting experiment: ctx3_L256-128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 82ms/step - accuracy: 0.0611 - loss: 7.7614\n",
            "Epoch 2/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.0824 - loss: 6.9330\n",
            "Epoch 3/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.0979 - loss: 6.6821\n",
            "Epoch 4/4\n",
            "\u001b[1m1664/1664\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 82ms/step - accuracy: 0.1055 - loss: 6.5093\n",
            "Finished ctx3_L256-128_do0.3 in 559.0s. Last-epoch loss=6.4633\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon were no real to be the ones you should be not the place of the world s longer could have\n",
            "    [i remember] -> i remember me i know to be on the city a poet s head and children s two two people are on\n",
            "    [in the morning] -> in the morning s silence the world do not live to watch the grass a coal of the broad town of his face\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon s matter this was time i am able for it will show to it a rest the force rig off\n",
            "    [i remember] -> i remember you with my heart put among the blood of this monster in the new while you tumble off achilles first\n",
            "    [in the morning] -> in the morning s in the to credit his hands have been in abraham the plain of status the einstein is very slippery\n",
            "================================================================================\n",
            "Starting experiment: ctx5_L128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 82ms/step - accuracy: 0.0637 - loss: 7.6925\n",
            "Epoch 2/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.0899 - loss: 6.8272\n",
            "Epoch 3/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 81ms/step - accuracy: 0.1032 - loss: 6.5669\n",
            "Epoch 4/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.1115 - loss: 6.3558\n",
            "Finished ctx5_L128_do0.1 in 554.9s. Last-epoch loss=6.2957\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon s hoods of the sun s sake when he was cold i am the world that is a man and\n",
            "    [i remember] -> i remember the long to sit in the world we aren t know to be i would not see the sun and\n",
            "    [in the morning] -> in the morning and the ripple and the universal stairs and the town of stunningly as our own self and i stand in\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon with salt he austere me in last and last the great virgin in the closet of tokyo 2 an animal\n",
            "    [i remember] -> i remember you died i asked me where i must had not only be mistake behind life where she looked into the\n",
            "    [in the morning] -> in the morning we lay earth and here that i am works no longer come for the cradle of my idea to 1979\n",
            "================================================================================\n",
            "Starting experiment: ctx5_L128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 81ms/step - accuracy: 0.0628 - loss: 7.7037\n",
            "Epoch 2/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 81ms/step - accuracy: 0.0886 - loss: 6.8886\n",
            "Epoch 3/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 81ms/step - accuracy: 0.1004 - loss: 6.6618\n",
            "Epoch 4/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 81ms/step - accuracy: 0.1084 - loss: 6.4888\n",
            "Finished ctx5_L128_do0.3 in 553.2s. Last-epoch loss=6.4420\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon river and i am a fuller a way to know they will not be there that is a few of\n",
            "    [i remember] -> i remember not the blaze of the sun in my head there is a mirror and a friend and a poet in\n",
            "    [in the morning] -> in the morning i was a man in the world s a woman s mother and a little box the dog that is\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon swirled at the stems like electric pack and salads for the lovelies reforma here in the fan seeker did care\n",
            "    [i remember] -> i remember all were works and came in the heights over bird or iron rug and pleas d d from among the\n",
            "    [in the morning] -> in the morning rang live and part of there it s beauty s pain was a gold steed for the old vines its\n",
            "================================================================================\n",
            "Starting experiment: ctx5_L128-128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 82ms/step - accuracy: 0.0616 - loss: 7.7305\n",
            "Epoch 2/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 82ms/step - accuracy: 0.0816 - loss: 6.8899\n",
            "Epoch 3/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.0966 - loss: 6.6408\n",
            "Epoch 4/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.1036 - loss: 6.4608\n",
            "Finished ctx5_L128-128_do0.1 in 563.3s. Last-epoch loss=6.4106\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon that watch the sea and the cloud of the river i remembered my love i am like a torch and\n",
            "    [i remember] -> i remember a man of the end of the water s in the sun of your head as the people is not\n",
            "    [in the morning] -> in the morning a sun and a old way to the people and the sweet one s net in the world s dress\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon s braided blood here is the teeth wounded so a bells of over the whole process of my broken word\n",
            "    [i remember] -> i remember therein enticed me from the sticking up the road arrived round these hungry eyes in sleep and oh you see\n",
            "    [in the morning] -> in the morning s shell his life s kitchen stopped as the creeks each old place of which s you find his chance\n",
            "================================================================================\n",
            "Starting experiment: ctx5_L128-128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 82ms/step - accuracy: 0.0609 - loss: 7.7449\n",
            "Epoch 2/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 82ms/step - accuracy: 0.0786 - loss: 6.9626\n",
            "Epoch 3/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.0941 - loss: 6.7397\n",
            "Epoch 4/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 82ms/step - accuracy: 0.1004 - loss: 6.5809\n",
            "Finished ctx5_L128-128_do0.3 in 550.7s. Last-epoch loss=6.5387\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon in the ships the mid quote a dance of the dark space of the world that did you have a\n",
            "    [i remember] -> i remember the other plain and when the dark road that you were worth it with the world on the right city\n",
            "    [in the morning] -> in the morning and a body is the year of my sleep i d d know me this is a poet as with\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon name called him or a man is much although i feel pulsed puts them off from the world the summer\n",
            "    [i remember] -> i remember and whatever look when i think because you left you to your backs and art it did you emerge twas\n",
            "    [in the morning] -> in the morning s first and no first look to the patience cancer and the dark or which planted a department could see\n",
            "================================================================================\n",
            "Starting experiment: ctx5_L256-128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 83ms/step - accuracy: 0.0603 - loss: 7.7692\n",
            "Epoch 2/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 83ms/step - accuracy: 0.0830 - loss: 6.8914\n",
            "Epoch 3/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 83ms/step - accuracy: 0.0991 - loss: 6.5971\n",
            "Epoch 4/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 83ms/step - accuracy: 0.1083 - loss: 6.3886\n",
            "Finished ctx5_L256-128_do0.1 in 554.4s. Last-epoch loss=6.3141\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon s no thing of the king of the weight of the same world s judgment which was what the same\n",
            "    [i remember] -> i remember the only story of a sentence but i will tell and i was to say in the sky and the\n",
            "    [in the morning] -> in the morning i will live in the ground the poet was fifty four and the woman is in the yellow harvest of\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon s our hands now how is an memorial that is the farthest feel and we were wonderingwhere and that the\n",
            "    [i remember] -> i remember in my own death not drop me by the indian council are the milkwhite seawater town in day our fact\n",
            "    [in the morning] -> in the morning he will break the wire in the spray cliff all at a provenanceof lit but if you rowed and that\n",
            "================================================================================\n",
            "Starting experiment: ctx5_L256-128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 83ms/step - accuracy: 0.0605 - loss: 7.7720\n",
            "Epoch 2/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 82ms/step - accuracy: 0.0804 - loss: 6.9715\n",
            "Epoch 3/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 83ms/step - accuracy: 0.0962 - loss: 6.7016\n",
            "Epoch 4/4\n",
            "\u001b[1m1651/1651\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 82ms/step - accuracy: 0.1038 - loss: 6.5150\n",
            "Finished ctx5_L256-128_do0.3 in 559.0s. Last-epoch loss=6.4588\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon between the morning and the same is a role of the dead i was a man of love and the\n",
            "    [i remember] -> i remember the way to you otherwise speak about me we are a way that she was so there s as a\n",
            "    [in the morning] -> in the morning s eye in a nurses in a way above the door and yet the new certain day is a series\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon train as will i dare you take but you could try worlds yourself and any day or even all what\n",
            "    [i remember] -> i remember the skies hath fruit as the song they both held me the rank on night like an dara supporters into\n",
            "    [in the morning] -> in the morning a few key to which my heart has lost in the beer he wear my eyes back in an testament\n",
            "================================================================================\n",
            "Starting experiment: ctx8_L128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 82ms/step - accuracy: 0.0637 - loss: 7.6516\n",
            "Epoch 2/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 82ms/step - accuracy: 0.0915 - loss: 6.8152\n",
            "Epoch 3/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 82ms/step - accuracy: 0.1045 - loss: 6.5487\n",
            "Epoch 4/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 82ms/step - accuracy: 0.1124 - loss: 6.3315\n",
            "Finished ctx8_L128_do0.1 in 544.5s. Last-epoch loss=6.2685\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon and all the cloud of the cold with the end the fire of the the sky and the one and\n",
            "    [i remember] -> i remember the man s need to be there is it s its heart she was a small man in a kind\n",
            "    [in the morning] -> in the morning and the wind and the sun who were the boy said she was to say the more than the world\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon box now and the bird comes or i would be not with waiting about you but on the night think\n",
            "    [i remember] -> i remember this sound comes from the ox of silence alone s rooms praise the conversation great earth s sparrows come to\n",
            "    [in the morning] -> in the morning in the masses the word we knew how would save the amorous school and human are that write nor they\n",
            "================================================================================\n",
            "Starting experiment: ctx8_L128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 82ms/step - accuracy: 0.0629 - loss: 7.6639\n",
            "Epoch 2/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 82ms/step - accuracy: 0.0894 - loss: 6.8789\n",
            "Epoch 3/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 82ms/step - accuracy: 0.1007 - loss: 6.6512\n",
            "Epoch 4/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 82ms/step - accuracy: 0.1097 - loss: 6.4777\n",
            "Finished ctx8_L128_do0.3 in 537.0s. Last-epoch loss=6.4273\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon there s a low garden s and a man s more than a kind of a search of the glass\n",
            "    [i remember] -> i remember the old town i didn t know it s the way to you you sit on the streets of your\n",
            "    [in the morning] -> in the morning the air and asked the froward sang the trees s shadow a second rate of the the people come to\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon clenching me to talk except there s un seconds i m signed the land but to love you is a\n",
            "    [i remember] -> i remember and my sluices will know because i dare not like good the rights of mark me you think that nothing\n",
            "    [in the morning] -> in the morning and hovers on the drawer he people am never thinking to all our case but it is mine to my\n",
            "================================================================================\n",
            "Starting experiment: ctx8_L128-128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 83ms/step - accuracy: 0.0616 - loss: 7.6997\n",
            "Epoch 2/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 82ms/step - accuracy: 0.0810 - loss: 6.8953\n",
            "Epoch 3/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 83ms/step - accuracy: 0.0957 - loss: 6.6545\n",
            "Epoch 4/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 82ms/step - accuracy: 0.1034 - loss: 6.4823\n",
            "Finished ctx8_L128-128_do0.1 in 543.0s. Last-epoch loss=6.4267\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon s o only a years of the word i want to send a other things for a way to turn\n",
            "    [i remember] -> i remember an gates of the heaven of the dark and the morning s face of the life i need to be\n",
            "    [in the morning] -> in the morning a rafters that the man was a imagination to the mind that i will come the sun in the road\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon s graders the black child a great heron finally happened d in human friends and the maker burning beneath the\n",
            "    [i remember] -> i remember the heavy tooth i think was the good time of the sky when are taught well is miraculous after you\n",
            "    [in the morning] -> in the morning air s arms of the even horses the changes still i say and his mother s lives maybe he is\n",
            "================================================================================\n",
            "Starting experiment: ctx8_L128-128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 83ms/step - accuracy: 0.0612 - loss: 7.7104\n",
            "Epoch 2/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 83ms/step - accuracy: 0.0782 - loss: 6.9618\n",
            "Epoch 3/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 83ms/step - accuracy: 0.0938 - loss: 6.7475\n",
            "Epoch 4/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 83ms/step - accuracy: 0.1001 - loss: 6.5921\n",
            "Finished ctx8_L128-128_do0.3 in 549.2s. Last-epoch loss=6.5472\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon over the cold blue skin of the floor to take an top of the wood of the mat s golf\n",
            "    [i remember] -> i remember the mind for the streets and be when the great child were more in the sky that is a poet\n",
            "    [in the morning] -> in the morning the moose dark itself in the ground a word that is to be th boy i d be a symbol\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon starts to windows the constellation of his french merry message a bush that autumn currency from their name with a\n",
            "    [i remember] -> i remember the muscle thick no bent the grown answering below that is a dear mother my back and the morning so\n",
            "    [in the morning] -> in the morning s semi drunks of her with a memory of the waves words in steel as vulnerable light the fragrance of\n",
            "================================================================================\n",
            "Starting experiment: ctx8_L256-128_do0.1  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 84ms/step - accuracy: 0.0605 - loss: 7.7420\n",
            "Epoch 2/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 84ms/step - accuracy: 0.0775 - loss: 6.9765\n",
            "Epoch 3/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 84ms/step - accuracy: 0.0957 - loss: 6.7065\n",
            "Epoch 4/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 84ms/step - accuracy: 0.1030 - loss: 6.5324\n",
            "Finished ctx8_L256-128_do0.1 in 560.1s. Last-epoch loss=6.4742\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon the night and the water that s not seen a poor hand on the things to the children my face\n",
            "    [i remember] -> i remember the man s own divide a day but there is one you didn t be been spoke to the children\n",
            "    [in the morning] -> in the morning while the world of my hand thou part of us and this s little name and the sky of a\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon either sound i made us deploy the middle of her mouth of wait to help more after they short too\n",
            "    [i remember] -> i remember that staid she sounded like a bottom by winter of a nightmare s deep meditation are taking in us your\n",
            "    [in the morning] -> in the morning and high hands unusually streets 4 made then every boy whose sweet way she include this ball for a woman\n",
            "================================================================================\n",
            "Starting experiment: ctx8_L256-128_do0.3  (epochs=4)\n",
            "Epoch 1/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 84ms/step - accuracy: 0.0606 - loss: 7.7493\n",
            "Epoch 2/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 83ms/step - accuracy: 0.0818 - loss: 6.9614\n",
            "Epoch 3/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 83ms/step - accuracy: 0.0963 - loss: 6.7273\n",
            "Epoch 4/4\n",
            "\u001b[1m1631/1631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 83ms/step - accuracy: 0.1030 - loss: 6.5580\n",
            "Finished ctx8_L256-128_do0.3 in 547.8s. Last-epoch loss=6.5083\n",
            "Sample generations:\n",
            "  -- temp0.6 --\n",
            "    [the moon] -> the moon all train and a embarked if i am waiting to be a girl to the day you are left to\n",
            "    [i remember] -> i remember the mirror of my love in the light in the world and the lives of the dark the valley of\n",
            "    [in the morning] -> in the morning sea who shall be did me the parts of a victim when i can not speak to a thing for\n",
            "  -- temp0.9 --\n",
            "    [the moon] -> the moon chase rising in me pulled the deck of som juice tossed of and i dream or no more enough for\n",
            "    [i remember] -> i remember rest with trains to the chance and the plaster voltage to the piers and not new man the soul are\n",
            "    [in the morning] -> in the morning o food and ago the voice the room of my snare between my boneless head and try of dusk and\n",
            "All experiments completed. Summary saved to lstm_experiments_summary.json\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import json\n",
        "\n",
        "# Experiment cell: try different LSTM sizes, dropout rates, and sequence lengths.\n",
        "# Uses existing variables/functions from the notebook: cleaned_poems, tokenizer, clean_text,\n",
        "# pad_sequences, _sample_from_preds. Does not overwrite the trained `model` variable.\n",
        "\n",
        "\n",
        "def prepare_sequences(context_len):\n",
        "  \"\"\"Create X,y for a given context length using the existing tokenizer & cleaned_poems.\"\"\"\n",
        "  token_seqs = []\n",
        "  for text in cleaned_poems:\n",
        "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
        "    if len(token_list) <= context_len:\n",
        "      continue\n",
        "    for i in range(context_len, len(token_list)):\n",
        "      seq = token_list[i - context_len : i + 1]  # length = context_len+1\n",
        "      token_seqs.append(seq)\n",
        "  if len(token_seqs) == 0:\n",
        "    raise ValueError(f\"No sequences created for context_len={context_len}. Reduce length or check data.\")\n",
        "  arr = np.array(token_seqs, dtype=np.int32)\n",
        "  Xp = pad_sequences(arr[:, :-1], maxlen=context_len, padding='pre')\n",
        "  yp = arr[:, -1]\n",
        "  return Xp, yp\n",
        "\n",
        "def build_lstm_model(vocab_size, context_len,\n",
        "                     embedding_dim=100, lstm_units_list=(128,),\n",
        "                     dropout_rate=0.2):\n",
        "  \"\"\"Builds and compiles an LSTM model with specified layer sizes.\"\"\"\n",
        "  m = Sequential()\n",
        "  m.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=context_len))\n",
        "  for i, u in enumerate(lstm_units_list):\n",
        "    # return_sequences for all but last LSTM\n",
        "    return_seq = (i < len(lstm_units_list) - 1)\n",
        "    m.add(LSTM(u, return_sequences=return_seq))\n",
        "  m.add(Dropout(dropout_rate))\n",
        "  m.add(Dense(vocab_size, activation='softmax'))\n",
        "  m.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return m\n",
        "\n",
        "def generate_from_model(seed, model_obj, context_len, num_words=20, temperature=1.0, sample=True):\n",
        "  # local generate using same preprocessing and sampling helper\n",
        "  cleaned = clean_text(seed)\n",
        "  token_list = tokenizer.texts_to_sequences([cleaned])[0]\n",
        "  generated = token_list.copy()\n",
        "  for _ in range(num_words):\n",
        "    input_seq = pad_sequences([generated[-context_len:]], maxlen=context_len, padding='pre')\n",
        "    preds = model_obj.predict(input_seq, verbose=0)[0]\n",
        "    if sample:\n",
        "      next_idx = _sample_from_preds(preds, temperature=temperature)\n",
        "    else:\n",
        "      preds[0] = 0.0\n",
        "      next_idx = int(np.argmax(preds))\n",
        "    generated.append(next_idx)\n",
        "  words = [idx_to_word.get(t, '<OOV>') for t in generated if t != 0]\n",
        "  return \" \".join(words)\n",
        "\n",
        "# Grid of experiments (kept small to run reasonably)\n",
        "sequence_lengths = [3, 5, 8]                      # try shorter/longer contexts\n",
        "lstm_configs = [(128,), (128, 128), (256, 128)]   # one vs two LSTM layers and sizes\n",
        "dropout_rates = [0.1, 0.3]\n",
        "embedding_dim = 100\n",
        "batch_size = 512\n",
        "epochs_per_run = 4   # small for fast iterations; increase when doing deeper experiments\n",
        "shuffle_buffer = 10000\n",
        "\n",
        "# seeds to inspect generated outputs\n",
        "seeds = [\"the moon\", \"i remember\", \"in the morning\"]\n",
        "\n",
        "results = []\n",
        "\n",
        "for context_len in sequence_lengths:\n",
        "  try:\n",
        "    Xp, yp = prepare_sequences(context_len)\n",
        "  except ValueError as e:\n",
        "    print(e)\n",
        "    continue\n",
        "\n",
        "  # build a tf.data pipeline once per context length\n",
        "  ds = tf.data.Dataset.from_tensor_slices((Xp, yp))\n",
        "  ds = ds.shuffle(shuffle_buffer).map(lambda x, y: (x, tf.one_hot(y, depth=vocab_size, dtype=tf.float32)))\n",
        "  ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  for lstm_cfg in lstm_configs:\n",
        "    for dr in dropout_rates:\n",
        "      cfg_name = f\"ctx{context_len}_L{'-'.join(map(str,lstm_cfg))}_do{dr}\"\n",
        "      print(\"=\"*80)\n",
        "      print(f\"Starting experiment: {cfg_name}  (epochs={epochs_per_run})\")\n",
        "      start = time.time()\n",
        "\n",
        "      exp_model = build_lstm_model(vocab_size=vocab_size,\n",
        "                                   context_len=context_len,\n",
        "                                   embedding_dim=embedding_dim,\n",
        "                                   lstm_units_list=lstm_cfg,\n",
        "                                   dropout_rate=dr)\n",
        "\n",
        "      hist = exp_model.fit(ds, epochs=epochs_per_run, verbose=1)\n",
        "\n",
        "      elapsed = time.time() - start\n",
        "      print(f\"Finished {cfg_name} in {elapsed:.1f}s. Last-epoch loss={hist.history['loss'][-1]:.4f}\")\n",
        "\n",
        "      # generate a few samples at different temperatures\n",
        "      samples = {}\n",
        "      for temp in (0.6, 0.9):\n",
        "        gen_lines = [generate_from_model(s, exp_model, context_len, num_words=20, temperature=temp, sample=True)\n",
        "                     for s in seeds]\n",
        "        samples[f\"temp{temp}\"] = gen_lines\n",
        "\n",
        "      # print concise outputs\n",
        "      print(\"Sample generations:\")\n",
        "      for temp, lines in samples.items():\n",
        "        print(f\"  -- {temp} --\")\n",
        "        for s, line in zip(seeds, lines):\n",
        "          print(f\"    [{s}] -> {line}\")\n",
        "\n",
        "      results.append({\n",
        "        'config': cfg_name,\n",
        "        'history': hist.history,\n",
        "        'samples': samples\n",
        "      })\n",
        "\n",
        "# Save experiment metadata to a file for later review (lightweight)\n",
        "with open(\"lstm_experiments_summary.json\", \"w\", encoding=\"utf-8\") as fh:\n",
        "  json.dump({\n",
        "    'sequence_lengths': sequence_lengths,\n",
        "    'lstm_configs': lstm_configs,\n",
        "    'dropout_rates': dropout_rates,\n",
        "    'results_summary': [\n",
        "      {'config': r['config'], 'last_loss': r['history']['loss'][-1], 'samples': r['samples']}\n",
        "      for r in results\n",
        "    ]\n",
        "  }, fh, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"All experiments completed. Summary saved to lstm_experiments_summary.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b900cc9d",
      "metadata": {},
      "source": [
        "## Report — Model Performance & Generated Poetry\n",
        "\n",
        "### Training & Experiments\n",
        "- The LSTM models (single- and multi-layer) trained on the sampled Poetry Foundation corpus and showed consistent learning behavior: loss decreased over epochs for most configurations, with larger models converging more slowly but often achieving lower final loss.\n",
        "- Shorter context lengths (3–5) produced faster training and more locally grammatical continuations; longer contexts (8) increased model capacity requirements and yielded somewhat better long-range coherence at the cost of more training time.\n",
        "- Two-layer LSTMs and larger hidden sizes improved expressiveness but increased overfitting risk when training epochs or data were limited.\n",
        "\n",
        "### Quantitative observations\n",
        "- Final loss and accuracy varied by config; simpler models (fewer units / shorter context) trained fastest and generalized better when data/epochs were limited.\n",
        "- Batch one-hot encoding in the tf.data pipeline kept memory manageable; expanding vocabulary or one-hot depth will require more memory or a different loss approach (sparse targets).\n",
        "\n",
        "### Qualitative observations of generated poetry\n",
        "- Local fluency: generated lines typically respect local word order and basic syntax (articles, prepositions, short phrases).\n",
        "- Repetition & generic phrases: the model often repeats common phrases or falls back to safe, high-frequency tokens, especially at higher temperatures or when seed context is weak.\n",
        "- Coherence: meaningful thematic continuity across many tokens is limited — the model tends to drift or introduce unrelated tokens after ~10–20 words.\n",
        "- OOV / rare words: rare poetic vocabulary is often replaced by common alternatives or the OOV token treatment, reducing poetic richness.\n",
        "- Temperature effects: lower temperature yields conservative, more grammatical output; higher temperature increases creativity but also incoherence and nonsensical tokens.\n",
        "\n",
        "### Practical recommendations / next steps\n",
        "- More data or data augmentation: train on a larger corpus (or use more of the dataset) to better capture poetic vocabulary and rare constructions.\n",
        "- Longer training & checkpoints: increase epochs and save checkpoints to choose models with the best validation/perplexity rather than final epoch.\n",
        "- Tokenization improvements: switch to subword tokenization (Byte Pair Encoding, WordPiece) to handle rare words and reduce OOVs.\n",
        "- Model architecture: experiment with Transformer-based language models (GPT-style) or bidirectional encoders for richer context; try increasing embedding & LSTM sizes cautiously.\n",
        "- Regularization & sampling: add weight decay, increase dropout, or use scheduled sampling; tune temperature and consider top-k / nucleus (top-p) sampling instead of pure temperature sampling.\n",
        "\n",
        "Overall: the current LSTM pipeline produces locally fluent and sometimes evocative lines but struggles with sustained poetic coherence and rare vocabulary. Improvements in data, tokenization, model capacity, and sampling strategies will yield the best gains."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
